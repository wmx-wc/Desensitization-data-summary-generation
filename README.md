# Desensitization-data-summary-generation
影像学 NLP — 医学影像诊断报告生成

## 介绍

该项目是由中国人工智能学会举办的一个比赛，主要任务是根据医生对 CT 的影像描述文本数据（即对医学影像特征的描述），生成诊断报告文本

## 数据处理

### 1. 脱敏数据处理

1. 原始数据为脱敏数据，即一个汉字由一个数字ID代替（共1290个不同的）。所以预训练模型的Tokenizer可能作用不大，而且大的词表训练的代价也高。
2. 我尝试了直接根据原始文本进行分词，发现分词的结果出现了歧义，比如1111代号的汉字被编码成两个11代号的汉字。
3. 更改了思路，采用不分词的做法，直接将原始文本传入模型的embedding层，模型的输入即为预测的结果。省去了Tokenizer步骤，缺点是没有利用词频信息。
4. 下一步做法是将数字ID映射为唯一的一个中文字，然后根据分词算法（BPE等）构造专属词表。

### 2. 动态掩码策略

1. 对比了MLM Mask和Span Mask的性能，发现Span Mask的CIDER-D高0.16
2. 下一步尝试使用n-gram mask策略

## 模型搭建

1. 采用facebook的Bart-Base模型，同时对比了Bart-Large模型，发现大模型在DAE预训练的第一个epoch已经过拟合，分析原因为数据量较少，大模型的维度的深度较高，容易过拟合
2. 当采用对抗训练训练方法时，模型在验证集出现了发散情况，一方面是FGM在攻击了位置编码层，另一方面是768的token维度与1300的词表大小相近，类似于维度灾难，解决办法是将模型的维度减小到512或者256

## 预训练

采用DAE预训练方法，将描述文本和报告文本拼接起来，采用掩码策略掩蔽一些输入。模型的编码器接收掩蔽后的输入，解码器接收右移一位的掩蔽后输入，输出被掩码的真实标签

## 多任务微调

采用MLM Loss和LM Loss的加权和进行微调，缓解模型过拟合现象

## 正则化

1. EMA
2. weight decay

## 代码运行

### 预训练

train文件夹下，运行pretrain.py即可

### 微调

fune文件夹
